{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "import pandas as pd  # Import pandas to handle structured data (CSV files)\n",
    "\n",
    "# Load dataset containing news articles\n",
    "df = pd.read_csv(\"dataset/data.csv\")  \n",
    "\n",
    "# Display basic information about the dataset (column names, data types, missing values, etc.)\n",
    "print(df.info())  \n",
    "\n",
    "# Display the first few rows of the dataset to check its structure\n",
    "print(df.head())  \n",
    "\n",
    "# We first load the data to see what weâ€™re working with. \n",
    "# It includes headlines, news content, topics, and labels showing whether the news is real or fake.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "import re  # Regular expressions for text cleaning\n",
    "import string  # String operations to remove punctuation\n",
    "import nltk  # Natural Language Toolkit (NLTK) for text processing\n",
    "from nltk.corpus import stopwords  # Import stop words to remove unnecessary words\n",
    "from nltk.tokenize import word_tokenize  # Tokenization (splitting text into words)\n",
    "from nltk.stem import WordNetLemmatizer  # Lemmatization (converting words to their root form)\n",
    "\n",
    "# Download necessary NLTK data files (only needs to be run once)\n",
    "nltk.download('punkt')  # Tokenizer data\n",
    "nltk.download('stopwords')  # Stopwords data (common words like \"the\", \"is\", etc.)\n",
    "nltk.download('wordnet')  # WordNet dictionary for lemmatization\n",
    "\n",
    "# Initialize tools\n",
    "stop_words = set(stopwords.words('english'))  # Set of stopwords in English\n",
    "lemmatizer = WordNetLemmatizer()  # Initialize lemmatizer to get base form of words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Convert text to lowercase to maintain consistency\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers since they don't contribute to meaning\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove all punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "    tokens = word_tokenize(text)  # Tokenize the text (split into words)\n",
    "    tokens = [word for word in tokens if word not in stop_words]  # Remove stop words\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]  # Convert words to their base form\n",
    "    return \" \".join(tokens)  # Join words back into a single string\n",
    "\n",
    "# Apply text cleaning function to 'title' and 'text' columns, then combine them\n",
    "df['cleaned_text'] = df['title'] + \" \" + df['text']\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(clean_text)\n",
    "\n",
    "# Print the first few cleaned rows to verify text preprocessing\n",
    "print(df['cleaned_text'].head())\n",
    "\n",
    "# We clean the text by making everything lowercase, removing unnecessary words, \n",
    "# and simplifying words to their root form. This helps our model focus on important words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4 Split Data for Training and Testing\n",
    "from sklearn.model_selection import train_test_split  # Import function to split dataset\n",
    "\n",
    "# Define features (text) and target variable (label: 0 = fake, 1 = real)\n",
    "X = df['cleaned_text']\n",
    "y = df['label']\n",
    "\n",
    "# Split dataset into 80% training and 20% testing to evaluate model performance\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")  # Print number of training samples\n",
    "print(f\"Testing samples: {len(X_test)}\")  # Print number of testing samples\n",
    "\n",
    "# We split the data so the model can learn from one part and be tested on another.\n",
    "# This helps us check if it works on new articles.\n",
    "\n",
    "#5\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # Import TF-IDF vectorizer\n",
    "\n",
    "# Convert text into numerical features using TF-IDF (Term Frequency - Inverse Document Frequency)\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # Keep top 5000 most relevant words\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)  # Transform training text into numerical data\n",
    "X_test_tfidf = vectorizer.transform(X_test)  # Transform test text using the same vectorizer\n",
    "\n",
    "print(\"TF-IDF matrix shape:\", X_train_tfidf.shape)  # Print shape of TF-IDF matrix\n",
    "\n",
    "# We turn words into numbers! TF-IDF gives higher importance to words that are rare and meaningful.\n",
    "\n",
    "## 6 Train Random Forest Model\n",
    "from sklearn.ensemble import RandomForestClassifier  # Import Random Forest classifier\n",
    "\n",
    "# Train a Random Forest classifier to classify real vs. fake news\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)  # Use 100 decision trees\n",
    "model.fit(X_train_tfidf, y_train)  # Train the model\n",
    "\n",
    "# Test the model and print accuracy\n",
    "accuracy = model.score(X_test_tfidf, y_test)\n",
    "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# We train a machine learning model using Random Forest to recognize patterns in the text \n",
    "# and predict if an article is fake or real. Then we test how well it performs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get predictions on the test set\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Display confusion matrix\n",
    "plt.figure(figsize=(6,5))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Fake News (0)\", \"Real News (1)\"])\n",
    "disp.plot(cmap=\"Blues\", values_format=\"d\")\n",
    "plt.title(\"Confusion Matrix - TF-IDF + Random Forest\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7\n",
    "# Load new articles for validation\n",
    "val_df = pd.read_csv(\"dataset/validation_data.csv\")\n",
    "\n",
    "# Clean the validation text\n",
    "val_df['cleaned_text'] = (val_df['title'] + \" \" + val_df['text']).apply(clean_text)\n",
    "\n",
    "# Convert validation text to TF-IDF format\n",
    "X_val_tfidf = vectorizer.transform(val_df['cleaned_text'])\n",
    "\n",
    "# Predict fake (0) or real (1) labels\n",
    "val_df['label'] = model.predict(X_val_tfidf)\n",
    "\n",
    "# Replace any remaining label 2 values with 0 (fake)\n",
    "val_df['label'] = val_df['label'].apply(lambda x: 0 if x == 2 else x)\n",
    "\n",
    "# Save predictions to a new CSV file\n",
    "val_df.to_csv(\"dataset/validation_predictions.csv\", index=False)\n",
    "\n",
    "print(\"Predictions saved successfully!\")\n",
    "\n",
    "# We clean and process new articles, then use our trained model to predict if they are fake or real. \n",
    "# Finally, we save the results!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#8\n",
    "from sklearn.metrics import classification_report  # Import function to check model performance\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "# Print classification report with precision, recall, and F1-score\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# We check how well the model performs using accuracy, precision, and recall. \n",
    "# These numbers tell us how reliable our fake news detector is!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra to show more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#9\n",
    "from collections import Counter  # Import Counter to count word occurrences\n",
    "import matplotlib.pyplot as plt  # Import Matplotlib for plotting\n",
    "\n",
    "# Get most common words in real and fake news articles\n",
    "real_words = \" \".join(df[df[\"label\"] == 1][\"cleaned_text\"]).split()\n",
    "fake_words = \" \".join(df[df[\"label\"] == 0][\"cleaned_text\"]).split()\n",
    "\n",
    "real_counts = Counter(real_words).most_common(15)  # Top 15 words in real news\n",
    "fake_counts = Counter(fake_words).most_common(15)  # Top 15 words in fake news\n",
    "\n",
    "# Plot most common words in real vs. fake news\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.bar(*zip(*real_counts), color='blue', label=\"Real News\")\n",
    "plt.bar(*zip(*fake_counts), color='red', alpha=0.7, label=\"Fake News\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Most Common Words in Fake vs. Real News\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Fake news often uses dramatic words, while real news tends to have more neutral wording. \n",
    "# This chart shows the most frequent words in both categories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud  # Import WordCloud to generate visual representations of text data\n",
    "\n",
    "# Generate Word Clouds for Real and Fake News\n",
    "real_wc = WordCloud(width=800, height=400, background_color=\"white\").generate(\" \".join(real_words))  \n",
    "fake_wc = WordCloud(width=800, height=400, background_color=\"black\").generate(\" \".join(fake_words))\n",
    "\n",
    "# Create a figure for displaying both word clouds\n",
    "plt.figure(figsize=(12,5))  \n",
    "\n",
    "# Display the Real News word cloud\n",
    "plt.subplot(1,2,1)  # Create a subplot (1 row, 2 columns, 1st plot)\n",
    "plt.imshow(real_wc, interpolation=\"bilinear\")  # Show the word cloud image\n",
    "plt.axis(\"off\")  # Remove axis labels for cleaner display\n",
    "plt.title(\"Real News Word Cloud\")  # Add title\n",
    "\n",
    "# Display the Fake News word cloud\n",
    "plt.subplot(1,2,2)  # Create a subplot (1 row, 2 columns, 2nd plot)\n",
    "plt.imshow(fake_wc, interpolation=\"bilinear\")  # Show the word cloud image\n",
    "plt.axis(\"off\")  # Remove axis labels for cleaner display\n",
    "plt.title(\"Fake News Word Cloud\")  # Add title\n",
    "\n",
    "# Show the final plot with both word clouds\n",
    "plt.show()\n",
    "\n",
    "# Shows most used words in real vs. fake news\n",
    "# Helps detect patterns in language use\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer  # Import sentiment analysis tool\n",
    "\n",
    "# Download sentiment analysis tool\n",
    "nltk.download(\"vader_lexicon\")  \n",
    "sia = SentimentIntensityAnalyzer()  \n",
    "\n",
    "# Compute sentiment scores for each article\n",
    "df[\"sentiment\"] = df[\"cleaned_text\"].apply(lambda text: sia.polarity_scores(text)[\"compound\"])  \n",
    "\n",
    "# Plot sentiment distribution\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.hist(df[df[\"label\"] == 1][\"sentiment\"], bins=20, alpha=0.7, label=\"Real News\", color=\"blue\")  \n",
    "plt.hist(df[df[\"label\"] == 0][\"sentiment\"], bins=20, alpha=0.7, label=\"Fake News\", color=\"red\")  \n",
    "plt.title(\"Sentiment Distribution in Fake vs. Real News\")  \n",
    "plt.xlabel(\"Sentiment Score\")  \n",
    "plt.ylabel(\"Count\")  \n",
    "plt.legend()  \n",
    "plt.show()\n",
    "\n",
    "# Measures the emotional tone of articles\n",
    "# Shows if fake news is more negative or emotional\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier  # Import Random Forest model\n",
    "from sklearn.linear_model import LogisticRegression  # Import Logistic Regression model\n",
    "from sklearn.naive_bayes import MultinomialNB  # Import Naive Bayes model\n",
    "from sklearn.metrics import accuracy_score  # Import accuracy evaluation\n",
    "\n",
    "# Define models to test\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=500),  # Increased iterations for better convergence\n",
    "    \"Naive Bayes\": MultinomialNB()\n",
    "}\n",
    "\n",
    "# Train and evaluate each model\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_tfidf, y_train)  # Train model\n",
    "    y_pred = model.predict(X_test_tfidf)  # Make predictions\n",
    "    accuracy = accuracy_score(y_test, y_pred)  # Calculate accuracy\n",
    "    results[name] = accuracy  \n",
    "    print(f\"{name} Accuracy: {accuracy:.2f}\")  \n",
    "\n",
    "# Plot model comparison\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.bar(results.keys(), results.values(), color=[\"blue\", \"green\", \"orange\"])\n",
    "plt.title(\"Model Comparison\")  \n",
    "plt.ylabel(\"Accuracy\")  \n",
    "plt.ylim(0.9, 1.01)  # Adjust y-axis to highlight high accuracy\n",
    "plt.show()\n",
    "\n",
    "# Compares Random Forest, Logistic Regression, and Naive Bayes\n",
    "# Removes SVM to speed up training while keeping high accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tfidf_vectorizer.pkl']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save trained model\n",
    "joblib.dump(model, \"random_forest_model.pkl\")\n",
    "\n",
    "# Save TF-IDF vectorizer\n",
    "joblib.dump(vectorizer, \"tfidf_vectorizer.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ironhack10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
