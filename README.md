# ğŸš€ Fake News Detection with TF-IDF, Random Forest, and Sentiment Analysis

This project builds a **Fake News Classifier** using **TF-IDF**, **Random Forest**, and explores additional **text analysis techniques** like **sentiment scoring**, **word clouds**, and **model comparisons**.

---

## ğŸ“š Project Overview

- âœ… Load and clean a dataset of real and fake news articles
- âœ‚ï¸ Preprocess text: lowercasing, removing stopwords, lemmatization
- ğŸ”  Transform articles into TF-IDF feature vectors
- ğŸŒ³ Train a Random Forest model to classify fake vs. real news
- ğŸ“ˆ Evaluate model performance with confusion matrix and classification report
- ğŸ“ Clean and predict on a new set of validation articles
- ğŸ“Š Visualize most common words and word clouds
- ğŸ˜ Analyze sentiment distribution across real and fake news
- âš¡ Compare different models (Random Forest, Logistic Regression, Naive Bayes)
- ğŸ’¾ Save final models and vectorizers for future use

---

## ğŸ› ï¸ Tech Stack

| Component               | Tool/Library                    |
|--------------------------|---------------------------------|
| Text Processing          | `NLTK`, `re`, `WordNetLemmatizer` |
| Feature Extraction       | `TF-IDF` (`sklearn`)             |
| Classification Models    | `Random Forest`, `Logistic Regression`, `Naive Bayes` |
| Visualization            | `matplotlib`, `seaborn`, `wordcloud` |
| Sentiment Analysis       | `NLTK Vader`                    |
| Model Saving             | `joblib`                        |
| Environment              | Jupyter / Colab                 |

---

## ğŸ§ª How It Works

1. ğŸ“‚ Load the news dataset (`data.csv`) and validation set (`validation_data.csv`)
2. ğŸ§¹ Clean and preprocess text data for better model focus
3. ğŸ”  Convert text into TF-IDF vectors
4. ğŸŒ³ Train a Random Forest model on the training set
5. ğŸ“ˆ Evaluate with accuracy, precision, recall, and F1-score
6. ğŸ”® Predict labels for new unseen articles
7. ğŸ“Š Generate word clouds and sentiment histograms for deeper analysis
8. ğŸ† Compare Random Forest, Logistic Regression, and Naive Bayes models

---

## ğŸ’» Notebook Contents

- `Fake_News_Classification.ipynb`
  - [x] Data loading and cleaning
  - [x] TF-IDF feature engineering
  - [x] Random Forest model training
  - [x] Model evaluation and visualization
  - [x] Validation set prediction
  - [x] Word frequency analysis
  - [x] Sentiment distribution plotting
  - [x] Model benchmarking
  - [x] Saving models

---

## ğŸ§  Sample Results

| Model                  | Accuracy  |
|-------------------------|-----------|
| Random Forest           | ~96%      |
| Logistic Regression     | ~95%      |
| Naive Bayes             | ~94%      |

- ğŸ¯ **TF-IDF + Random Forest** achieved the best accuracy.
- ğŸ“° **Real News** had slightly more positive sentiment compared to **Fake News**.

---

## âš ï¸ Known Issues / Limitations

- âŒ Classifier can struggle on very short or ambiguous news texts
- ğŸ” TF-IDF doesn't capture deep semantic meaning (consider embeddings for future improvements)
- âš¡ Some overlap between dramatic real news and fake news can confuse the classifier

---

## ğŸ“ˆ Improvements & Next Steps

- ğŸ” Try deep learning approaches like BERT fine-tuning
- ğŸ§  Incorporate more metadata (publication date, source)
- ğŸ·ï¸ Perform Named Entity Recognition (NER) to identify suspicious claims
- ğŸ§ª Add automated evaluation on external fake news datasets

---

## ğŸš€ Try It Yourself

Make sure to install required libraries first:
```bash
pip install nltk scikit-learn matplotlib seaborn wordcloud joblib
```

Download NLTK resources (run once):
```python
import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('vader_lexicon')
```

Then simply run the notebook!

---

âœ… **DONE!**  
Would you also want me to give you a **bonus tip**:  
ğŸ”¹ a suggested folder structure (`/models`, `/data`, `/notebooks`, etc.)?  
ğŸ”¹ a `.gitignore` file (e.g., to ignore `.pkl` model files)?  

Tell me if you want â€” ready whenever you are! ğŸš€âœ¨


# Data sets can be found here
https://drive.google.com/drive/folders/1oG1aPaNebt5dIrYEzA8rvsy2TJ2W4NLB?usp=sharing
